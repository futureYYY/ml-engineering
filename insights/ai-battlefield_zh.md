# ai-battlefield - 中文翻译

## AI战场工程——你需要知道的一切

本章是某人对ML/AI工程现实的主观概述，这可能是也可能是另一人的现实。目的是帮助你开始提出正确的问题，并满足你的ML工程需求。

## 基础知识

### 在AI竞赛中什么是重要的？

训练：

1. 能多快训练出更好的模型（抢占市场优势）
2. 花费了多少钱（训练后我们是否还有钱支付人才工资）

推理：

1. 快速延迟（用户习惯于毫秒级响应时间，如果响应需要几秒他们会离开）
2. 高吞吐量（一次能处理多少并发查询）
3. 每个用户花费多少钱（我们能否租用更多的GPU来获取更多用户和/或改善（1）和（2））

### 大型语言模型训练的需求是什么？

1. 快速计算，主要由矩阵乘法主导
2. 快速足够的内存、IO、网络和CPU以支持计算

推论：如果你在购买或租赁硬件时投资最快的加速器，但吝啬于其他组件，那么你可能浪费了资金，且无法赢得比赛，因为训练时间会更长。

### ML工作的主力是什么？

- 加速器或处理单元是做大部分工作的工具。

- 由于ML进行了大量并行处理（SIMD），一开始使用了GPU，但现在还有TPU、IPU、FPGA、HPU、QPU、RDU等。最近的CPU也开始被用作加速器，尤其是在推理方面。

[更多信息](../compute/accelerator)。

### 驱动AI的实体

- AI公司——围绕自训练或他人的训练模型开发产品，在内部进行研究。
- 学术界——进行大规模研究并撰写论文。许多新思想由此产生。
- AI爱好者——有大量好的意愿，有些人聚集资源/人才来训练开源模型，通过捐赠的计算资源和偶尔的云服务，或大学集群。
- 创业者——有很多低垂的果实可摘——创造性地转售服务，创建ML驱动的应用程序，并利用各种巧妙组合现有的资源创造惊人的成果。

### 信息共享

- 几乎所有涉及AI的人都与社区分享大量的发现，这非常令人惊讶。
- 当然，公司不会披露所有的知识产权，但很多以知识或模型权重的形式被分享。
- 发布大量知识产权和模型的公司往往吸引更高素质的人才。
- Twitter似乎是必须关注的中心平台，以便了解最新动态。

### AI泡沫

- 互联网泡沫发生在1995-2000年。现在AI领域也出现了类似的情况。

- 有很多资金可用于创建新的初创企业或提升现有公司。筹集数百万美元相对容易。

- 由于我们正处于AI行业的狂野西部阶段，很难预测未来，因此只要听起来合理，几乎任何创业想法都可以尝试。

- AI泡沫与互联网泡沫的区别在于，运营互联网公司实际上不需要太多资金——大多数筹集的资金都用于营销和员工，几乎没有用于计算。AI公司需要数百万美元，因为训练大型语言模型需要大量的计算，而这种计算非常昂贵。例如，1个NVIDIA H100的成本约为3万美元，公司可能需要512个这样的设备，总计1500万美元（不包括其他硬件组件及相关成本）！

## ML工程师的天堂和地狱

这是我个人基于LLM/VLM训练的天堂和地狱。具体情况可能有所不同。

### ML工程师的天堂

1. 一个构建良好的HPC，或全服务的基于云的集群，有人勤勉及时地维护硬件和系统。

   我只需要带上我的训练软件并进行训练，这已经是一个非常复杂的工作，需要特殊技能。

2. 可用的节点数量充足，可以无限独占使用

3. 快速的节点间连接，不会限制加速器的速度，也不与其他用户共享

4. 大容量的本地超级快速NVME共享文件系统，可以容纳数据集和检查点

5. 极简的Linux系统，配有SLURM和少量软件以启动训练任务

6. `sudo`权限以方便团队协作


### ML工程师的地狱

1. 云或内部集群，你需要自己完成所有工作——系统管理、更换硬件、处理断电等。并且还要进行训练。

2. 小型慢速共享文件系统（如NFS？），从云中抽取数据和保存检查点

3. 节点间连接缓慢导致加速器利用率低

4. 与其他用户共享的节点使网络变得不稳定和不可预测

5. 超级复杂的云控制台，需要多个屏幕和步骤才能设置简单的事情

6. 不能快速替换故障硬件

7. 需要与其他用户共享节点——训练作业之间有等待时间

8. 其他并发用户可能会用完整个磁盘，导致训练崩溃

9. 无法终止团队中其他人开始并已进入睡眠状态的任务

## 获取计算资源

有三种主要选择：

- 租用云资源
- 分享HPC资源
- 购买硬件

### 租用云资源

目前这是获取计算资源的主要方式。

优点：

- 容易扩展或缩小集群规模
- 容易在几年内升级到新一代硬件
- 集群管理可以轻松外包

缺点：

- 除非你与供应商签订长期（1-3年）合同以获得数百个加速器，否则成本高昂
- 你会被诱惑购买许多可能并不需要的工具和服务
- 不管你是否充分利用集群，你都会被收费

### 使用HPC

HPC的数量有限，因此可用资源也很有限。

优点：
- 为你管理一切——你只需要自己的软件来进行训练，以及一点点SLURM知识来启动任务
- 经常由当地政府或大学赞助——可能以较低的价格甚至免费完成任务（例如，我们在JeanZay HPC上免费训练了BLOOM-176B！）

缺点：
- 需要与其他团队共享计算资源——可能导致短作业时间和长时间等待——可能难以快速完成训练
- 节点间的网络可能不稳定，因为会被其他团队使用
- 必须遵守HPC的规则（例如，没有sudo权限和各种其他规则）
- 在某种程度上，HPC集群将是它本来的样子——你无法使其网络更快，而且安装某些软件通常很棘手。

### 购买硬件

主要是大学购买和构建自己的集群，一些大公司也会这样做。

优点：

- 如果你能连续多年全天候运行硬件，总体成本将比租赁便宜
- 容易提供快速本地存储——一个好的NVME RAID比在线存储更便宜更快

缺点：

- 你可能会在购买几年后就被淘汰的硬件所困——可能能够转售
- 必须购买超过所需——硬件容易损坏，尤其是当它们全天候运行时，返修可能需要数周
- 必须聘请人才来管理内部解决方案
- 必须解决冷却、电力成本、保险等问题

## 技术需求

### 你能把煤铲进炉子吗？

想象一下蒸汽机车——引擎很棒，但如果司炉工铲煤不够快，火车就不会开得快。

![](images/640px-Baureihe52Heizer.jpg)

[来源](https://commons.wikimedia.org/wiki/File:Baureihe52Heizer.jpg)

这是当前ML硬件的状态：瓶颈在于移动比特而不是计算。

- 加速器每两年约提高2倍速度（摩尔定律）
- 网络和内存却不是！目前两者都是计算瓶颈
- 如果你的DataLoader必须从云中拉取数据，IO也可能成为另一个瓶颈
- CPU只要拥有足够多的cpu核心供DataLoader工作，以及主进程就足够了

推论：研究整个机器，而不仅仅是它的引擎。

一个疯狂的想法：如果你能实际喂入加速器的计算速度一样快，较旧的GPU可能也能胜任。如果你能在相同成本下得到三倍的旧GPU，你可能会更快完成训练并以更低的成本完成。

### TFLOPS

- 一旦选择了架构、模型大小以及你希望训练模型的令牌数量，你就立即知道需要多少计算来实现这一目标。具体来说，你现在可以计算出需要多少浮点运算（[如何计算所需的TFLOPS](../training/performance/README.md#tflops-as-a-performance-metric)）。

- 所缺少的是比较不同的计算提供商，看看他们的硬件每秒可以执行多少浮点运算（TFLOPS）及其单位成本，这样你现在就可以估算出大致的训练总成本。

  1. 根据考虑解决方案的TFLOPS计算所需的时间：
     `总TFLOPS需求 / 这个计算单元的TFLOPS = 秒数`
     假设结果为604800秒或7天。

  2. 查看使用这个计算方案7天的成本，你现在就知道了训练这个模型的总费用。

  3. 查看其他方案并进行相同的计算——选择最佳选项。

- 如前所述，时间非常重要，所以即使选择更昂贵的方案，如果尽快完成训练很重要，因为你想率先上市，这也是合理的。

不幸的是，这种数学计算只是部分正确，因为公布的峰值TFLOPS通常是无法达到的。接下来的部分将进一步探讨MFU。

### 模型浮点运算利用率（MFU）

正如前一节所述，一些（大多数？）供应商发布的峰值性能TFLOPS是不现实的——它们不可能实现。

模型浮点运算利用率（MFU）是衡量加速器利用率的指标。以下是计算方法：

1. 通过计算单次训练迭代需要多少浮点运算并除以该迭代所花费的时间来测量实际TFLOPS。
2. 将实际TFLOPS除以公布的TFLOPS得到MFU

举例说明：假设你在BFLOAT16精度下训练：

- 如果单次迭代需要624万亿次浮点运算，并且运行时间为4秒，那么我们知道得到：`624/4=156` 实际TFLOPS
- 现在BF16@A100的公布值为312TFLOPS，所以 `156/312=0.5` 给我们50%的MFU。

实际上：
- 对于使用NVIDIA GPU的多节点设置和大型模型，如果你的MFU超过50%，你已经做得很好了
- 最近在更高效的可扩展性解决方案上的进展不断提高了MFU
- 慢速网络和效率低下的框架或未调优的配置降低了MFU

因此，一旦你知道了MFU，你现在可以调整上一部分中的成本估算。在我们的例子中，我们说需要7天来训练，但如果MFU是50%，这意味着需要14天来训练。

### 移动比特

为什么无法实现公布的TFLOPS？这是因为数据在加速器内存和计算之间移动需要时间，此外从磁盘和其他GPU向加速器内存移动数据需要更多时间。

- 对于加速器内存，带宽是固定的，你可以只通过编写更高效的软件来加快数据的传输速度 - 提示：融合和自定义编写的内核（如[torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html) 和 [flash attention](https://github.com/Dao-AILab/flash-attention))

- 如果你只有一个GPU，且模型适合其内存，你不必担心网络问题——加速器内存是唯一的瓶颈。但是，如果你需要将模型分片到多个GPU上，网络就会成为瓶颈。

- 节点内网络——非常快，但难以充分利用大型模型 - [张量并行化](../training/model-parallelism#tensor-parallelism) 和 [序列并行化](../training/model-parallelism#sequence-parallelism) 解决了部分问题。([更多信息](../network/README.md#intra-node-networking))。

- 节点间网络——在大多数服务器设置中通常太慢——因此这是关键的研究对象！高效的框架通过重叠计算和通信部分隐藏通信开销。但如果通信时间超过计算时间，通信仍然是瓶颈。[更多信息](#inter-node-network)。

- 存储IO对于喂养DataLoader工作者和保存检查点非常重要。[更多信息](#storage)。

  1. 通常有足够的DL工作者，DataLoader几乎不会增加额外开销。
  2. 当检查点正在保存时，除非使用某种异步保存解决方案，否则加速器将处于空闲状态，因此快速IO至关重要这里

## 关键硬件组件

### 加速器

截至写作时，以下是最常见的可用于训练、微调和推理ML模型的加速器：

广泛可用：

- NVIDIA H100s正在逐渐取代A100s。我们希望H200s能很快取代H100s（Q4-2024），因为后者具有更高效的HBM，因此更具成本效益。

受限但可用：

- Google TPUs——速度快！但代价是锁定在一个单一供应商和云服务中。

即将普及：

- NVIDIA H200——比H100更快的HBM和更多内存——Q4-2024在一些选定的云服务上（并非所有大型云服务都计划采购这些设备）。

- NVIDIA B100、B200和GB200——预计在2025年初上市。

- AMD MI300X ≈ H100——Tier 2云服务从Q2-2024起提供这些设备——需要使用最新的ROCm并激活许多优化才能获得高TFLOPs。

- Intel Gaudi3 > H100——在Intel的云服务上可用。

- GraphCore IPU——非常难找，PaperSpace有售。

- Cerebras WaferScale Engine——在Cerebras的云服务上可用。

有关完整列表和最近宣布的加速器，请参阅[加速器](../compute/accelerator)。

#### 加速器互操作性

一般来说，大多数（所有？）加速器都得到了主要框架（如PyTorch或TensorFlow）的支持，只要代码不使用任何特定于加速器的功能，相同的代码应该可以在不同平台上运行，只需进行小的修改。

例如，如果你的PyTorch应用程序调用了`torch.mm`，它应该在任何地方都能正常工作，但如果它包含自定义的CUDA内核，它只会适用于NVIDIA GPU，并且可能适用于最近的AMD MI系列。

- NVIDIA GPU：所有基于[CUDA](https://developer.nvidia.com/cuda-toolkit)，大多数训练框架都支持。你可以轻松地在不同的NVIDIA GPU之间切换，大多数事情都能正常工作。

- AMD MI250/MI300X：使用[ROCm](https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/)，你可以在大多数CUDA软件上运行PyTorch。这是唯一与NVIDIA堆栈互通的加速器。

- Intel Gaudi2/Gaudi3：如果你使用HF Transformers/Diffusers，你可以使用[optimum-habana](https://github.com/huggingface/optimum-habana)。如果你使用HF Trainer与NVIDIA GPU，应该相对容易切换到Gaudi2进行训练/推理。

- GraphCore IPU：也可以通过[poptorch](https://github.com/graphcore/poptorch)在PyTorch中运行。

- Cerebras：也在通过[XLA](https://www.cerebras.net/blog/supporting-pytorch-on-the-cerebras-wafer-scale-engine/)提供PyTorch支持。

总的来说，大多数ML代码可以编译成跨平台格式，如[开放神经网络交换（ONNX）](https://en.wikipedia.org/wiki/Open_Neural_Network_Exchange)，可以在各种加速器上运行。这种方法通常更多用于推理工作负载。

### 网络

- 如果你要训练一个不能放在单个加速器内存中的大型模型，你必须依靠节点内和节点间的网络来同步多个加速器。

- 目前最大的问题是计算硬件的发展速度超过了网络硬件的发展速度，例如对于NVIDIA NVLink节点内的（单向带宽）：

| GPU  | 计算<b>fp16<br>TFLOPS | 计算<br>速度提升 | 节点内<br>GBps | 节点内<br>速度提升 |
| :--- |                      --: |                --: |                --: |                   --: |
| V100 |                      125 |                  1 |                150 |                     1 |
| A100 |                      312 |                2.5 |                300 |                     2 |
| H100 |                      989 |                  8 |                450 |                     3 |
| B200 |                     2250 |                 18 |                900 |                     6 |

- 你可以看到，A100比V100快2.5倍，而H100比A100快约3倍。但是节点内的NVLink带宽每一代仅增加了150GBps。NVLink 5.0比NVLink 4.0快两倍，所以稍微跟上了计算速度的提升。但速度提升仍然不足。

- 此外，前四代NVLink使用相同的NIC，带宽为25GBps单向。它们只是通过增加链接的数量来提速。所以在那个技术上没有进展。

- 节点间的网络情况也没有好多少，大多数NIC的速度为100或200Gbps，一些新兴的是400Gbps。（相应地为12.5、25和50GBps）。情况类似，一些解决方案提供了数十个NIC以达到更高的速度。

- 通常对于LLMs，有效载荷非常大，训练中的网络延迟往往可以忽略不计。但对于推理来说仍然相当重要。

#### 节点内网络

- 注意字节和位的区别。1字节=8位。1GBps=8Gbps。

- 如果需要在多个节点间减少比特（例如梯度），最慢的链接（节点间）决定了整体吞吐量，因此节点内的速度并不重要。

- [张量并行化](../training/model-parallelism#tensor-parallelism)和[序列并行化](../training/model-parallelism#sequence-parallelism)必须在节点内保持高效——只有在快速的节点内速度下才有意义。

NVIDIA:

- 基于NVIDIA的计算节点配备50GBps双工NVLink

- 有些节点有很多NVLink，有些较少，但通常至少有450GBps（3.6Tbps）单向带宽，H100为300GBps，A100节点为300GBps

Intel Gaudi2:

- 8 x 21个100GbE RoCE v2 ROMA NIC，总计2.1TBps

[更多信息](../network/README.md#intra-node-networking)

#### 节点间网络

- 比节点内网络慢一个数量级

- 你会看到从50Gbps到3200Gbps的广泛速度范围

- 你需要比计算更快地减少梯度和其他比特，以避免加速器空闲

- 你最多只能得到广告速度的80%。例如，如果你被告知你得到800Gbps，预期大约为640Gbps。

- 如果转换到fp8，H100比V100快18倍

- 我们还没有看到3200Gbps对H100是否足够以保持高MFU。

* 实际上不到3倍，这是一个不错的估计

[更多信息](../network/README.md#inter-node-networking)

### 存储

在ML工作负载中有三个不同的存储IO需求：

1. 你需要能够快速喂食DataLoader——（快速读取，不关心快速写入）——需要持续数小时至数天的可持续负载
2. 你需要能够快速写入检查点——（快速写入，快速读取，因为你将恢复几次）——需要突发写入——你想要超快以避免长时间阻塞训练（除非你使用某种CPU卸载以快速解除阻塞训练）
3. 你需要能够加载和维护你的代码库——（中等速度的读写）——这还需要共享，因为你想让所有节点看到相同的代码库——因为它只在开始或恢复时发生，所以很少发生

- 大多数情况下，你所购买的存储资源只有80%是可靠的。如果你需要一个可靠的100TBs，你需要租用125TBs，否则你的应用程序可能在磁盘满之前就无法写入。

- 共享分布式文件系统：
  1. 非并行共享文件系统在你有许多小文件时可能会非常慢（=Python！）
  2. 你想要并行文件系统，如GPFS（IBM Spectrum Scale）或Lustre（开源）

[更多信息](../storage/README.md)

### CPU内存

你需要足够的内存来：

- 每个加速器需要2-3个可能的DL工作者（因此8个加速器每个节点有16-24个进程）

- 如果你从云端拉取数据，则需要更多内存给DL工作者

- 如果不能直接加载到加速器上，需要足够的内存来加载模型

- 经常用于加速器内存卸载——通过交换当前未使用的层来扩展加速器的内存——如果是这个目的，那么CPU内存越多越好