# README - 中文翻译

节点间与节点内网络硬件

**子章节**:

- [网络调试](debug)
- [网络基准测试](benchmarks)

## 简介

仅仅购买或租用昂贵的加速器来快速训练和推理模型是不够的。您还需要确保您的存储IO、CPU和网络足够快，以“喂饱加速器”。如果无法确保这一点，则昂贵的加速器将被低效利用，导致金钱损失、训练时间延长和推理吞吐量降低。虽然可能任何上述组件都可能出现问题，但在训练过程中，网络往往是瓶颈（假设您的DataLoader已经很快）。 

如果您的模型可以装在单个加速器上，那么您几乎无需担心。然而，如今大多数模型需要多个加速器来加载数据，并且LLM/VLM模型需要多个计算节点来进行训练，有些甚至需要多个节点进行推理。

大多数计算节点包含8个加速器，有些有4个，有些有16个，甚至更多。最近也有一些节点配置为每个节点有一个超级加速器。

当模型分布在多个加速器上且不超出单个节点时，您只需要关注快速的[节点内网络](#节点内网络)。一旦模型需要多个节点（这在训练中通常是情况，因为可以使用多个副本并行化和加速训练），则快速的[节点间网络](#节点间网络)成为关键。

本文涵盖了这两种类型的网络硬件，报告它们的理论和有效带宽，并解释它们如何相互作用。

## 术语表和概念

您可以安全地忽略这里列出的许多概念和缩写，直到您需要时再回来查看。

- ALU：算术逻辑单元
- AR：自适应路由（也可能是聚合路由器）
- DMA：直接内存访问
- EFA：弹性结构适配器
- HCA：主机通道适配器
- IB：InfiniBand
- MFU：模型浮点利用率（例如，在A100半精度上MFU=0.5来自获得156TFLOPs，因为峰值半精度规格是312TFLOPS，因此`156/312=0.5`）
- NIC：网络接口卡
- OPA：Omni-Path架构
- OPX：Omni-Path Express
- OSFP：八通道小型可插拔（收发器）
- RDMA：远程直接内存访问
- RoCE：通过融合以太网的RDMA
- RoE：通过以太网的RDMA
- SHARP：可扩展分层聚合减少协议
- VPI：虚拟协议互连
- xGMI：插槽到插槽全局内存接口

速度相关：
- 单向：从一个点到另一个点的单向传输 A -> B
- 双向、全双工：从一个点到另一个点的双向传输 A <-> B，通常为单向速度的两倍
- GBps、GB/s：每秒传输的吉字节数 (1GBps = 8Gbps)
- GT/s：每秒传输次数 - 每秒发生的传输数据操作数。
- Gbps、Gb/s：每秒传输的吉比特数 (1Gbps = 1/8GBps)
- 割集宽度：分割网络成两个部分所需的最小链路数（不一定相等）。这些链路的带宽称为割集带宽——这通常用于衡量实际网络带宽）。有时它被称为最坏情况下的网络容量。这是一个[好的答案](https://networkengineering.stackexchange.com/a/29662/93656)，它解释了这个和其他概念，但除非您需要了解细节，否则您很可能不需要理解这些概念，因为很可能您的集群拓扑已经由供应商完成。
- 自适应路由改进静态路由，使数据包可以无序传输。数据包在每个交换机处负载均衡，以便更好地分配网络工作负载。
- [远程直接内存访问](#rdma-网络)

脚注：在以下部分中，请注意1GBps = 8Gbps。

### 单向与双向（全双工）

大多数基准测试/带宽测量工具会报告单向带宽。所以请注意单向与双向（全双工）速度之间的差异。通常后者大约快两倍。

如果您在设置上测量的带宽约为广告速度的40％，请仔细检查广告速度是否表示全双工，如果是这样，请将其减半，然后您的测量带宽应约为80％，这是预期的结果。

案例研究：有一段时间我无法理解为什么在运行nccl-tests的all_reduce基准测试时，我在具有600GBps节点内速度的A100节点上只能得到235GBps（40％），直到Horace He善意地指出我应该查看单向速度，即300GBps，然后我得到了理论规范的80％，这符合预期。

## 集群网络

集群中的每个节点都有三种不同速度的网络：

1. [前端网络](#前端网络)
2. [后端网络](#后端网络)
3. [带外网络](#带外网络)

### 前端网络

前端网络通常用于互联网连接（例如下载Python包和上传到云存储）、分布式网络存储（例如检查点和数据集）以及编排（例如SLURM和Kubernetes）。目前，典型的节点很可能会有一个100-400Gbps的连接。

脚注：并非所有集群都会有外部互联网连接可用，例如许多高性能计算环境仅提供通过特殊CPU节点的外部访问。

### 后端网络

后端网络用于GPU到GPU的连接，这使得训练和推理可以扩展到多个加速器（例如，全减少、全聚集和其他集体通信）。这是AI集群最重要的部分。通常，这将是[InfiniBand](#infiniband)或[RoCEv2以太网](#rdma-网络)之一。然后分为[节点内网络](#节点内网络)和[节点间网络](#节点间网络)——同一节点上的GPU通常可以比与其他节点上的GPU通信更快。目前，典型的最高速度为节点内约5600Gbps，节点间每节点约3200Gbps。每个加速器至少会有一个后端连接，有时一个加速器会有多个连接，特别是如果使用的是低带宽NIC。

脚注：并非所有供应商都会匹配行业标准的网络速度——在某些情况下，节点间的网络速度可能慢至10倍。所以一定要检查您得到的是什么。

### 带外网络

带外（OOB）网络用于启动后端网络、监控节点健康状况、远程重新映像节点等。它通常使用一个单一的较慢的1Gbps以太网连接。

## RDMA网络

远程直接内存访问就像节点内的直接内存访问，但跨越节点。它允许跨节点进行数据交换而无需使用本地处理器、操作系统内核和缓存，这是TCP/IP所使用的。主要有三个实现：

1. InfiniBand
2. 通过融合以太网的RDMA (RoCE) (基于IB或UDP的RDMA)
3. iWARP (基于TCP的RDMA)

这是一个[好的概述文章](https://community.fs.com/article/roce-vs-infiniband-vs-tcp-ip.html)。