# README - 中文翻译

## 推理

XXX：本章正在建设中——有些部分已完成，有些部分刚开始，还有很多部分尚未开始，但已经完成的有用部分足以使其成为一篇不错的阅读材料。

## 词汇表

- CLA：跨层注意力
- FHE：全同态加密
- GQA：分组查询注意力
- ITL：令牌间延迟
- KV：键值
- LPU：语言处理单元™
- MHA：多头注意力
- MPC：安全多方计算
- MQA：多查询注意力
- PPML：保护隐私的机器学习
- QPS：每秒查询数
- TPOT：每输出令牌的时间
- TTFT：首次令牌时间

参见[概念](#概念)获取更多类似词条。

## 概念

### 预填充和解码

在进行推理时有两个阶段：

#### 预填充

预填充：由于所有提示令牌已知，一次性处理完整的提示长度（类似于训练），并缓存中间状态（KV缓存）。这个阶段贡献的延迟非常小，即使是一个包含1000个提示的模型也可以快速处理，前提是内存足够。

#### 解码

解码：生成新的令牌，一次生成一个新令牌（回归方法），基于之前的所有令牌（提示和迄今为止生成的所有新令牌）。因此，这个阶段对生成的延迟贡献最大，因为与预填充不同，解码不能并行化。

### 在线推理与离线推理

当用户实时发送查询时，这是在线推理，也称为部署或交互式推理。示例包括聊天机器人、搜索引擎、通用REST API。在这种情况下，总是运行一个推理服务器，并且可能会有各种客户端对其进行查询。

当你有一个包含数百或数千个提示文件需要进行推理时，这是离线推理。示例包括基准评估和合成数据生成。在这种情况下，通常不需要推理服务器，推理直接在发送查询的同一程序中运行（客户端和服务端在一个应用程序中）。

两种主要使用场景通常优化不同的性能指标——在线推理需要非常低的TTFT和低延迟，而离线推理则需要高吞吐量。

### 上下文提供

这是给预训练模型提供其训练期间不可用的额外信息的过程。
例如，[输入上下文化任务](#输入上下文化任务)在提示中为模型提供了大量额外信息。非零样本提示通过示例改变默认模型行为来上下文化模型。提示工程完全是为了在推理过程中使模型表现特定的方式。

检索增强生成（RAG）是上下文化模型的主要技术之一，因为它为推理过程提供了与提示相关的附加数据。目标是让模型比它所训练的大规模压缩信息更重视这些信息。

微调到不同的知识领域是另一种上下文化方法，我们更新模型以适应新的数据集，该数据集可能与基础模型训练的数据领域截然不同。

上下文可以看作是提供背景。任何人都可以证明，理解问题的背景更容易回答问题。这同样适用于模型生成。背景越好，生成的输出越相关。

在多模态使用案例中，与文本提示一起提供的图像或视频可以作为背景或上下文。

### 任务

#### 输入上下文化任务

输入上下文化任务是指生成的回答主要来自提示，即知识的主要来源包含在提示中。这些包括：

- 翻译
- 摘要
- 文档问答
- 多轮对话
- 代码编辑
- 语音识别（音频转录）

### 批处理

逐个令牌地处理解码阶段效率极低。批处理多个查询可以提高加速器利用率，并能够同时处理多个请求。

最大可能的批处理大小取决于加载模型权重后剩余的内存量以及填充KV缓存所需的状态。

#### 静态批处理

这是一种简单的批处理方式，首先N个查询被批处理在一起——这里的问题是，如果许多查询已经完成了生成，它们将不得不等待最长的生成查询完成，然后才能返回给调用者——大大增加了延迟。

#### 连续批处理或飞行中批处理

连续批处理或飞行中批处理是一种过程，生成引擎在生成结果完成后立即移除已完成的结果，并用新的查询替换它们，而不必等待整个批次完成。因此，批次中的第0个序列可能正在生成它的第10个令牌，而批次中的第1个序列可能刚刚开始生成第一个令牌，第3个位置正在生成最后一个令牌。

这种方法改善了响应时间，因为已经完成的序列无需等待立即返回，新的提示也不必等待下一个批次可用。当然，如果所有的计算都完全繁忙，并且没有新的空间在批次中，则某些请求必须等待计算开始处理它们。

### 分页注意力

分页注意力在推理服务器中非常流行，因为它通过分页的方式利用加速器内存非常高效，就像操作系统使用分页管理内存一样，允许动态内存分配并防止内存碎片。

### 解码方法

主要的解码方法包括：[贪婪解码](#贪婪解码)，[束搜索](#束搜索) 和 [采样](#采样)。

#### 贪婪解码

贪婪解码是指模型总是选择概率最高的令牌。这是最快的解码方法，但它并不总能生成最佳结果，因为它可能会选择不太理想的令牌路径而错过一个伟大的未来令牌序列。

这种方法的主要问题之一是循环创建，其中相同的句子会一遍又一遍地重复。

#### 束搜索

束搜索通过同时生成多个输出克服了贪婪解码的限制，因此不是遵循最高概率，而是使用束大小为3的情况下，在每个新的令牌上跟随前3个概率，然后从9个子路径（`3*3`）中保留导致总概率最高的3个子路径。最后，选择所有令牌中总概率最高的路径。

这种方法比贪婪解码慢，因为它必须生成n倍更多的令牌，并且需要n倍的内存。

#### 采样

基于采样的解码引入了随机性。

但是，当然，选择随机词不会产生好的结果，所以我们仍然希望像贪婪解码那样具有一定的确定性，但通过添加受控的随机性使其更有趣/生动。

最常见的采样方法是：

- **Top-K采样**方法基于其对数概率选择前k个令牌，然后从中随机选择一个令牌。
- **Top-p采样**（也称为**核采样**）与Top-K采样类似，但K对于每个下一个令牌都会变化，通过累加顶部令牌概率直到达到阈值`p`来计算。所以如果有预测模型非常确定的情况，只有那些才会被考虑。

#### 温度

温度是另一种影响[Top-p](#采样)采样策略的组成部分，其影响根据其值如下：

- `t==0.0:` 最终会选择概率最高的令牌——没有随机性——与贪婪解码相同——精确应用场景。
- `t==1.0`：对采样没有影响——保留原始训练分布——平衡相关性和多样性应用场景。
- `0.0<t<1.0`：对数概率进一步拉开距离，因此越接近0.0，随机性越少——介于精确和平衡应用场景之间。
- `t>1.0`：对数概率拉近距离，产生大量随机性——创造性应用场景。

为了真正理解影响，温度因子通常在应用Softmax操作之前或作为其一部分应用于对数概率。

``` 
logits = math.log(probs) / temperature
```
可以看出`t=1.0`没有任何影响，`t=0`会使最高对数趋向无穷大（避免了零除），`t<1.0`和`t>1.0`会相应地将值分开或拉近——因为`log`。

温度对贪婪解码、束搜索和Top-K采样策略没有影响，因为它影响对数概率之间的距离，所有这些策略都基于它们的顺序使用最高概率，而温度不会改变概率的顺序。而Top-p采样允许更多或更少的竞争对手进入随机抽样将从中抽取的子集，因此概率越接近（高温），可能的随机性越大。

除了`t==0.0`和`t==0`之外，没有硬性的规定值可以复制，你需要为每个应用场景实验找到最适合你的需求的值——尽管如果你在网上搜索，肯定会找到针对不同应用场景的良好基线。

### 引导文本生成

也称为结构化文本生成和辅助生成。

如果模型能够以特定格式返回其生成的输出，而不是不受限的格式，那么你就不希望模型产生无效格式。例如，如果你想让模型返回一个JSON字典，那么它就应该这样做。

实现这一点的方法是使用引导文本生成。与其选择具有最高概率的生成令牌，该技术使用符合下一个预期令牌子集的下一个最佳概率令牌。举个例子来说：如果你希望模型生成一个字符串列表，如`["apples", "oranges"]`，那么我们期望：

``` 
["string", "string", ..., "string"]
123...
```

第一个生成的令牌必须是`[`。如果模型得到`"`，例如，作为最高概率，而`[`的概率较低，那么我们希望选择概率较低的那个，即`[`。

接下来生成的令牌必须是`"`。如果不是，搜索概率较低的令牌直到找到`"`并选择那个。

第三个令牌必须是一个有效的字符串（即不是`[`或`"`）。

依此类推。

基本上，对于每个下一个令牌，我们需要知道一组允许的令牌，并从该集合中选择概率最高的一个。

这是一个非常酷的技术。与其尝试修复生成的输出，这在匹配预期格式时并不总是可能的，我们让模型一开始就能生成正确的输出。

这项技术有几个成本：
- 它会减慢生成速度——遵循的模式越复杂，生成令牌的速度就越慢。从测量生成速度来看，我发现一些结构化文本生成库的性能比其他库快得多。
- 它可能会增加模型幻觉的风险。

目前有多种实现方式，截至撰写本文时，两个流行的库是：
- https://github.com/outlines-dev/outlines
- https://github.com/noamgat/lm-format-enforcer

你最好希望这些实现已经被集成到像vLLM这样的推理框架中。

#### 使用模式加速推理

还可以使用模式来加速推理。例如，考虑以下简单的“个人资料”模式：

``` 
{
  "type": "object",
  "properties": {
    "name": { "type": "string"},
    "age": { "type": "integer"}
  },
  "required": ["name", "age"]
}
```

由于模式具有特定的键`name`和`age`，一旦模型预测出`{"n`或`{"a`，就不需要进行自动回归生成来得出`{"name": `和`{"age": `，因为这两个都必须导致一个特定的无歧义单一结果——因此这里可以执行预填充而不是解码，从而节省几个缓慢的步骤，因为它知道接下来的几个令牌将会是`ame": `或`ge":`。

显然，当模式具有大量预先确定的键并且生成的值较短时，这种方法最为有益。

### 假设推理

也称为假设推理或辅助生成。

由于逐个生成令牌非常慢，有时可以通过使用更小更快的草稿模型来作弊并加快速度。例如，你的正常推理使用Llama-70B，这会相当慢，但我们可以用Llama-7b作为草稿模型，然后可以通过一次性处理所有令牌来验证预测是否正确。

举例来说：假设我们有一个提示`I'm turnin', turnin', turnin', turnin', turnin' around and all that I can see is just`，现在：

1. 使用Llama-7b逐个自回归预测`another lemon tree`，但在3步内，比Llama-70b快得多。
2. 现在使用Llama-70b运行3个提示的批量：

``` 
[...I can see is just]
[...I can see is just another]
[...I can see is just another lemon]
```
为了演示我缩短了完整的提示，使用了`...`，在实际中应该在那里。并且我在这里假设每个令牌是一个完整的单词。

现在Llama-70B在一步内生成：

``` 
[...I can see is just] another
[...I can see is just another] lemon
[...I can see is just another lemon] tree
```

现在可能有多个结果：
- 如果一切匹配——在3个短步骤和1个长步骤中生成最终结果，而不是使用3个长步骤。
- 如果只有`another lemon`匹配——我们可能还是更好，因为它节省了时间。
- 如果没有或只有一点点匹配——我们浪费了一点时间。

显然，如果代替3个令牌我们有更多的令牌，节省的可能是更大的。

还要注意的是，相比正常情况下使用大型模型进行此生成，这里我们实际上进行了更多的计算，但由于这种方法的延迟可能要好得多——所以用户平均应该从使用这种方法的应用程序中获得更好的响应时间——如果草稿模型较小但仍能产生良好的预测。

当存在部分不匹配时，我们可以回到草稿模型，将其与第一个不匹配令牌之前的匹配令牌一起，以及由大模型预测的下一个好的令牌，让它快速预测不匹配的尾部。

草稿模型理想上应使用相同的训练数据（或至少来自相似分布的数据），并且其分词器必须与大型模型相同。

假设推理在[输入上下文化任务](#输入上下文化任务)中效果最好，例如翻译、摘要、文档问答、多轮对话，因为在这些任务中可能的输出范围更小，草稿模型更有可能与大型模型匹配。

出于同样的原因，当用于[贪婪解码](#贪婪解码)时效果最好，因为生成过程中可能的变化最少。如果不使用贪婪解码，你将希望[温度](#温度)值接近0。

这里是关于这个主题的一个深入探讨：[辅助生成：一种新的低延迟文本生成方向](https://huggingface.co/blog/assisted-generation)。

另一种更简单的解决方案是使用[ngram提示查找解码](https://github.com/apoorvumang/prompt-lookup-decoding)。在这种方法中，无需草稿模型，而是搜索提示以生成候选。在某些情况下，据说它可以将解码速度提高2倍以上。

### 保护隐私的推理

大多数提供推理服务的公司都会遇到用户隐私的需求。用户提交查询应该是安全的，不应该有人窥探。一种解决方案是在本地部署，客户自己运行服务器，这样就没有隐私问题了，但这很可能暴露提供商的IP——模型权重和可能的代码/算法。因此，需要一种完全加密的生成——即计算在客户端加密的数据上进行。

解决这一需求的解决方案被称为保护隐私的机器学习（PPML）。

其中一个解决方案是全同态加密（FHE）。

看看这样一个实现，[concrete-ml](https://github.com/zama-ai/concrete-ml)，它重写模型，使得客户端可以自行运行模型的一部分，然后中介加密激活会被发送到服务器以执行注意力，然后再发送回客户端。因此，提供商保留了部分知识产权——我认为这部分知识产权防止客户端窃取完整知识产权，因为部分权重不足以重建整个模型。[这篇文章](https://huggingface.co/blog/encrypted-llm)对此进行了详细说明。

还有其他各种方法，例如这篇论文：[LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers](https://arxiv.org/abs/2305.18396v3)提出了一个基于安全多方计算（MPC）和FHE的定制解决方案，并附有一份很好的参考文献。

当前解决方案的问题是巨大的计算开销——这极大地影响了成本和延迟。未来ASIC解决方案应该能够解决这些问题。

### 模型并行化

当模型无法在单个加速器上装下或者即使装下但勉强装下时，使用多个加速器拆分模型会更有效率，相同的[模型并行化技术](../训练/模型并行化)在推理中也适用。

#### 张量并行化

大多数时候你最有可能遇到的是[张量并行化](../训练/模型并行化#张量并行化)，其中模型权重分布在2到8个加速器上。理想情况下，你想尝试将模型装入单个加速器，因为这样在生成过程中开销最小。但令人惊讶的是，使用张量并行化后，解码吞吐量可能会更高——这是因为这使你能装入更大的批次，并且由于加速器间的`forward`调用可能更快，尽管增加了通信量。当然，你会为此付出更多加速器的代价。所以最好进行实验，有些使用案例在相同数量的加速器下，更高的张量并行化程度可能会带来更好的总吞吐量。

脚注：在我的实验中，TP=1导致最高的TTFT和最低的解码吞吐量，与TP>1相比。因此，如果你被要求加快TTFT速度并且模型装得下，请使用较小的TP或TP=1。如果你被要求加快解码吞吐量，请使用更多的加速器并提高TP级别。

#### 管道并行化

此外，虽然张量并行化有助于降低延迟，但使用[管道并行化](../训练/模型并行化#管道并行化)可以提高吞吐量。这在使用大量加速器来加载模型权重时尤其如此。例如，如果你使用的是Llama 405B并且TP=8，则每个加速器必须与其他7个加速器进行全减少，而使用PP=8时，每个加速器只需要与其他2个加速器通信（接收前一阶段的输入并发送当前输出到下一阶段），大大降低了网络层的压力，如果硬件支持，这可以大幅提速。

重要的是要澄清，只有在使用完整的PP而不是朴素的PP时，PP才能优于TP。在[朴素的PP](../训练/模型并行化#朴素的模型并行化垂直)中，每次只有一个PP阶段工作，因此性能会比TP差。为了从PP中受益，推理框架需要并行地喂入所有PP阶段以执行[完整的PP](../训练/模型并行化#管道并行化)。

关于PP推理的另一件重要事情是，与训练不同，没有`backward`传递，因此不需要解决不活跃气泡问题。在最初的几个微批次中只会有一点微小的填充PP阶段的开销。

与训练一样，你可能会发现一些TP和PP的混合组合会导致最好的结果（例如，对于Llama 405B，TP=4 + PP=4）。因此，请确保进行实验并测量不同的配置，选择满足你需求的最佳配置。

## 关键推理性能指标

有两种方式看待性能指标，系统指标如延迟和吞吐量，以及用户体验指标：首次令牌时间（TTFT）和每输出令牌时间（TPOT）。让我们来看看这两对指标。

### 系统性能指标

#### 延迟

**延迟是从请求发送到接收到完整响应的时间**。

这包括：
1. 接收请求的时间
2. 预处理提示（预填充阶段）
3. 生成响应的新令牌（解码阶段）
4. 将响应发送回客户端

接收请求和发送响应的时间因提示长度和生成响应长度的差异而略有不同。这些长度变化对总时间的影响可以忽略不计。

预填充阶段并行处理所有提示令牌，因此这里的长度变化也不会造成太大差异，尽管较长的提示会消耗更多的加速器内存并影响总吞吐量。

解码阶段受生成响应长度的影响最大，因为每个新令牌都是单独生成的步骤。这里，响应越长，解码阶段就会越长。

如果服务器没有足够的容量一次性处理所有当前请求，必须排队一些请求，那么队列中的等待时间会延长延迟。

脚注：如果你把交通想象成汽车，延迟就是从一个地点到另一个地点（例如家到办公室）的时间，包括交通灯、拥堵和法律限制导致的速度限制。

#### 吞吐量

吞吐量衡量推理服务器处理多个请求并在并行处理中高效批处理的能力。

吞吐量的定义可以由每秒可以并发处理的请求数量来定义，但由于某些请求处理得比其他请求快得多，因此在单个长请求期间可以处理多个短请求，所以有意义的是计算整个系统生成的令牌总速率。

因此，**推理吞吐量的常见定义是整个系统的每秒生成令牌总数**。

脚注：如果你把交通想象成道路上的车辆流量，吞吐量就是在任何给定时间通过给定道路的车辆数量。道路的车道越多，速度限制越高，道路的吞吐量就越高。但显然有些车辆短，有些车辆长，所以需要某种归一化。例如，渡轮会计算可以容纳多少米或英尺的车辆，因此长车支付的费用比短车多。

### 用户体验指标

虽然推理服务器可以从很多方面评判，比如功耗、效率和成本，但可以说，由于系统接口人类，最重要的特征都在用户体验的范畴内。如果用户体验缓慢且卡顿，用户会转向竞争对手。因此关键需求是：

#### 首次令牌时间

**首次令牌时间（TTFT）是指用户点击提交按钮（或Enter）以来，到他们收到第一个单词或部分单词的时间**。

非常低的首次令牌时间（TTFT）是需要的。如今用户习惯于期待任何应用程序的理想响应时间小于1秒。因此用户等待收到第一个令牌的时间越短越好。这对于预期互动的聊天机器人尤为重要。TTFT的长度受到许多因素的影响，最重要的是[预填充阶段](#预填充)（预处理提示）的计算，以及请求是否在用户请求到达时立即处理，或者是否必须等待在队列中。

值得注意的是，TTFT在服务器无负载时与服务器在高负载时的表现可能有很大不同。如果服务器通常在1秒内发送第一个令牌，而在服务器已经忙于处理所有它可以同时处理的请求并且有队列的情况下，除了最初几个请求外，有效的TTFT可能会明显更长。因此通常应该测量平均TTFT，并与在基准测试期间发送的并发请求数量一起报告。

这是一个非平凡的指标，因为根据提示大小时间会有所不同，所以最好将其归一化为提示中的令牌数量。

#### 每输出令牌时间

每输出令牌时间（TPOT）是一个用户指标。它测量为给定用户生成新令牌所需的时间。

相对较低的每输出令牌时间（TPOT）是需要的，但不必太高。这个时间理想上应该接近发送请求的人的阅读速度。例如，如果你服务于小学生，TPOT可以相当低，但教育程度越高的人，TPOT应该越快以实现流畅的阅读体验。

根据维基百科，有[三种阅读类型](https://en.wikipedia.org/wiki/Speed_reading#Types_of_reading)，阅读速度以每分钟单词数（WPM）来衡量。

每种分词器的平均每单词令牌数可能因词汇量大小和语言等因素而异。这里我们考虑一个英语分词器，大约每单词1.5个令牌。现在我们可以将每分钟单词数（WPM）转换为每分钟令牌数（TPM）。

然后我们只需将其除以60即可得到每秒令牌数（TPS），并取倒数以得到每输出令牌时间（TPOT）

所以 `TPOT = 60 / (WPM * 1.5)` 秒

| 读者     | WPM | TPM |  TPS |  TPOT |
| :-----   | --: | ---: | ----: | ----: |
| 默读     | 250 |  375 |  6.25 |  0.16 |
| 听读     | 450 |  675 | 11.25 | 0.089 |
| 视读     | 700 | 1050 | 18.75 | 0.057 |

记住将1.5系数更改为您的分词器的实际单词到令牌平均比例。例如，据我所知，截至撰文时，OpenAI ChatGPT的50k词汇量报告约为每单词1.3个令牌，而许多其他LLM有30k词汇量，这导致更高的令牌每单词比例。

正如您所见，TPOT是一个难以跟踪和思考的值，因此 **一旦你知道目标TPOT，最好将其转换为每秒令牌数（TPS）并跟踪该值**。

因此在这个例子中，如果您的系统每请求可持续生成20个令牌，那么您的客户会满意，因为该系统即使对于每分钟阅读700个单词的超级快速读者也能跟上。

当然，还会有用户希望在生成完成之前等待，然后才开始阅读回复。在这种情况下，越快越好。

根据生成类型，以下情况很可能会适用：
1. 图像 - 一次性生成
2. 文本 - 以用户阅读速度生成，如果他们不想在开始阅读之前看到移动部件，则一次性生成
3. 音频 - 以用户听觉速度生成
4. 视频 - 以用户观看速度生成

如果是不对接个体用户的离线系统，只是批量处理请求，这些指标无关紧要，但延迟和吞吐量是关键指标。

### 简化的性能指标

如你所见，上述讨论的指标有很多重叠。实际上，我们可以将它们简化为这两个指标：预填充吞吐量和解码吞吐量——可能还包括系统每秒可以处理多少个并行请求。

#### 预填充吞吐量

这是系统预处理提示的速度——每秒令牌数。

假设接收和发送请求的开销可以忽略不计，在没有队列的情况下，即接收到请求后立即开始处理，[TTFT](#首次令牌时间)实际上是提示中令牌数除以预填充每秒令牌数加上生成第一个令牌的时间（我们可以忽略，因为它会很快）。

如果有队列，那么预填充吞吐量不够，因为那么TTFT可能会更长，因为你需要加上请求在队列中等待的时间。

#### 解码吞吐量

这是系统生成响应令牌的速度——每秒令牌数。

这解决了吞吐量和每输出令牌时间（TPOT）指标。

响应延迟则是提示中的令牌数除以预填充吞吐量加上生成的令牌数除以解码吞吐量。

### 更多指标说明

#### 加速器利用率

加速器利用率——无论是百分比还是功率测量——都是一个很好的指标，表明你的设置是否有效地使用了加速器。例如，如果你使用的是NVIDIA GPU，并且你运行`watch -n 0.5 nvidia-smi`命令，并且看到你在大规模轰炸推理服务器时的GPU利用率仅为10%，这可能意味着推理服务器效率很低（例如，花费大量时间来回复制东西）或者客户端在接收数据时效率低下（即太多IO阻塞）。

脚注：当我第一次编写一个使用openai客户端的简单基准测试时，在低并发下工作正常，但在较高并发下，推理服务器的GPU利用率下降到6-7%。在我用`aiohttp` API替换了客户端之后，它上升到了75%。因此要注意，你的基准测试可能是导致性能不佳报告的罪魁祸首，而不是服务器本身。

这在某种程度上相当于使用[TFLOPS来衡量训练效率](../训练/性能/README.md#tflops作为性能度量)。

在理想情况下，你希望你的加速器利用率尽可能高。请注意，至少对于NVIDIA GPU，`gpu util` [并不是你想象的那样](../计算/加速器/nvidia/debug.md#如何获取真实的GPU利用率指标)，但如果它报告了一个非常低的百分比，那就足够表明肯定存在效率问题。

#### 百分位数

如果你阅读基准测试并遇到诸如p50、p75、p90、p95和p99百分位数之类的东西——这些是基于一定阈值下的结果过滤统计，给出基于一定百分比的结果。即使同一个请求在多次重新运行时也可能需要稍微不同的响应时间。因此，例如，如果95%的时间吞吐量高于某个值，那就是p95百分位数。这也意味着5%的时间吞吐量低于相同的阈值值。百分位数后面的数字越大，实现起来就越困难。

例如，让我们看一下由[k6](https://github.com/grafana/k6)生成的部分系统负载报告：

``` 
http_req_duration..: avg=13.74s   min=12.54s  med=13.81s   max=13.83s   p(90)=13.79s   p(95)=13.83s
http_req_receiving.: avg=27.98µs  min=15.16µs med=21.6µs   max=98.13µs  p(90)=44.98µs  p(95)=59.2µs
http_req_sending...: avg=133.8µs  min=20.47µs med=75.39µs  max=598.04µs p(90)=327.73µs p(95)=449.65µs
```

如果我们查看第一行报告总生成时间，如果我们查看记录的最小值12.54秒，那么我们知道90%的响应时间在12.54到13.79秒之间，95%的响应时间在12.54到13.83秒之间——在这个特定情况下，报告的中位数在p90和p95值之间。

相同的解释适用于报告中的其他行，但关键示例是p90值低于p95值，因为时间被测量（越低越好）。

当异常值不重要时，百分位数很有用，因此，例如，你可能会忽略最慢的吞吐量测量，而突然间系统性能看起来好了很多。但当你与用户打交道时要非常小心，因为这意味着一些用户会使用你的系统时体验不好。另外，5%相当于如果用户有数百万的话，那将是一个很大的数量。

请参阅[百分位数](https://en.wikipedia.org/wiki/百分位数)获取更深入的解释。

## 加快模型加载时间

在生产环境中，模型加载时间可能可以接受，因为它只发生一次，然后服务器运行几天，因此这个开销会在许多天中摊销。但在研究、开发和测试时，推理服务器启动服务的速度至关重要。

有时候，开销只是加载到CPU然后将张量移动到加速器，有时还需要额外的步骤来为多个加速器分片张量以进行[TP](../训练/模型并行化#张量并行化)和[PP](../训练/模型并行化#管道并行化)。

各种方法被用来解决这个问题——大多数涉及某种形式的预分片和缓存，随后直接加载到GPU。

例如：

- vLLM支持`--load-format`标志，可以选择选项如`npcache`（numpy格式缓存）或`tensorizer`使用CoreWeave的[Tensorizer](https://github.com/coreweave/tensorizer)。([食谱](https://docs.vllm.ai/en/latest/serving/tensorizer.html)，当然，如果你使用TP>1，你应该[一次性预分割权重](https://docs.vllm.ai/en/latest/getting_started/examples/save_sharded_state.html)。
- TensorRT-LLM要求用户为每个特定用例构建一个模型引擎，并在运行时加载预先制作的分片。除非你使用的是简化API，它会在每次服务器启动时动态构建模型引擎。

## 基准测试

你可以按照[关键推理性能指标](#关键推理性能指标)中所述编写自己的基准测试，也可以使用现有的基准测试。

目前我主要使用[预填充吞吐量](#预填充吞吐量)和[解码吞吐量](#解码吞吐量)基准测试。前者测量从请求发送并接收到第一个生成令牌的时间内的令牌每秒数，后者是在第一个和最后一个生成令牌接收到之间的时间内的吞吐量。以下是使用[`openai`客户端完成API](https://github.com/openai/openai-python)进行此类测量的相关代码片段：

``` 
[... 创建客户端、数据等 ...]
prefill_tokens_len = len(prompt)
start_time = time.time()
decode_text = ""
decode_started = False
completion = client.completions.create(prompt=prompt, ...)
for chunk in completion:
    if chunk.choices:
        decode_text += text
        if not decode_started:
            decode_started_time = time.time()
            prefill_time = decode_started_time - start_time
            decode_started = True

end_time = time.time()
decode_time = end_time - decode_started_time
decode_tokens = tokenizer.encode(decode_text)
decode_tokens_len = len(decode_tokens)

# tokens/per sec
prefill_throughput = prefill_tokens_len / prefill_time
decode_throughput  = decode_tokens_len  / decode_time
```

`prefill_throughput`在这里不是很精确，因为客户端只知道它何时发送请求和接收到第一个令牌，所以这个阶段比纯提示预处理多了一些时间，但它应该足够接近。

当然，像任何严肃的基准测试一样，你需要多次运行它以获得现实的数字，因为单次运行之间的差异可能相当大。

注：我发现当我使用openAI客户端时，它在高并发下扩展不佳，openAI客户端创建了一个瓶颈，无法测量真正的服务器性能——我还在调查这个问题，见https://github.com/vllm-project/vllm/issues/7935——我发现这个版本的客户端（https://github.com/vllm-project/vllm/blob/f842a7aff143a4a1ddc59e1fb57109cb377f5475/benchmarks/backend_request_func.py#L223-L301）重写为使用`aiohttp`后扩展得很好——所以我切换到了使用它。

这里有一些好的负载测试起点：

- https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_throughput.py - 我迄今为止最喜欢的工具
- https://github.com/grafana/k6 - 用于模拟多个并发客户端的负载测试 - 使用JavaScript客户端。
- https://github.com/bentoml/llm-bench - 基准测试推理负载（不确定是否仅适用于BentoML）

我现在缺少的是一个工具来测量服务器可以处理的最大并发量。

