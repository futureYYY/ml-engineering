# README - 中文翻译

## 加速器

计算加速器是机器学习训练中的主力军。最初只有GPU，但现在还有TPU、IPU、FPGA、HPU、QPU和RDU等，而且正在发明更多种类。

存在两种主要的机器学习工作负载——训练和推理。还有一种微调工作负载，通常与训练相同，除非执行非常轻量级的[LoRA风格](https://arxiv.org/abs/2106.09685)微调。后者需要的资源和时间显著减少。

在语言模型的推理过程中，生成是一次一个令牌进行的。因此，它需要重复数千次相同的“前向”调用，每次调用只涉及一个小规模的矩阵乘法（矩阵乘法或GEMM）。这可以在加速器上完成，如GPU，或者一些最新的CPU，它们能够高效地处理推理任务。

在训练过程中，整个序列长度通过一次巨大的矩阵乘法操作来处理。因此，如果序列长度为4000，则训练相同模型所需的计算单元必须能够处理比推理多4000倍的操作，并且速度要快得多。加速器在这方面表现出色。实际上，矩阵越大，计算效率越高。

另一个计算差异在于，虽然训练和推理在“前向”传递中需要执行相同数量的“矩阵乘法”，但在仅用于训练的“反向”传递中，为了计算输入和权重的梯度，需要额外执行两次“矩阵乘法”。如果使用激活函数重计算，还需要额外执行一次“前向”传递。因此，训练过程所需的“矩阵乘法”次数是推理的3到4倍。

## 子章节

通用：
- [基准测试](benchmarks)

英伟达：
- [解决英伟达GPU问题](nvidia/debug.md)

AMD：
- [解决AMD GPU问题](amd/debug.md)
- [AMD GPU性能](amd/performance.md)

## 高端加速器的概览

尽管未来可能会发生变化，但与消费级GPU市场不同，目前在云端租用时，大多数供应商提供的高端加速器选择相对较少。

GPU：
- 目前，机器学习云和高性能计算中心已经开始从NVIDIA A100转向H100，这个过程需要几个月的时间，因为NVIDIA GPU通常供应不足。H200预计将于2024年第四季度推出。B100、B200、GB200于2024年第一季度宣布，但由于生产延迟，我们可能要等到2025年中期才能使用这些产品。
- AMD的MI300X现在在二级云提供商中广泛可用。MI325X也即将上市。

HPU：
- 英特尔的Gaudi2正在英特尔的云中慢慢出现——有一系列的产品。还可以通过Supermicro、WiWynn等公司在本地部署。
- Gaudi3预计将在2024年的某个时候上市。

IPU：
- Graphcore提供了其IPU产品。您可以通过[Paperspace](https://www.paperspace.com/graphcore)尝试这些产品，通过他们的云笔记本。

TPU：
- 谷歌的TPU当然可用，但它们并不是最受欢迎的加速器，因为只能租用它们，并且软件在GPU和TPU之间不容易转换，所以许多（大多数？）开发者仍然停留在GPU领域，因为他们不想被锁定在一个由谷歌垄断的硬件上。

在Pod和机架上：
- Cerebras的晶圆级引擎（WSE）
- SambaNova的数据规模
- 数十种不同的Pod和机架配置，将上述GPU与超高速互连组合在一起。

截至2024年第三季度，这就是全部情况。

由于我们大多数人都租赁计算资源，而从未见过实际硬件的样子，以下是8xH100节点的物理外观（这是戴尔PowerEdge XE9680机架服务器的GPU托盘）：

![nvidia-a100-spec](images/8x-H100-node-Dell-PowerEdge-XE9680.png)


## 术语表

- CPU：中央处理器
- FPGA：现场可编程门阵列
- GCD：图形计算芯片
- GPU：图形处理单元
- HBM：高带宽内存
- HPC：高性能计算
- HPU：Habana Gaudi AI处理器单元
- IPU：智能处理单元
- MAMF：最大可实现矩阵乘法浮点运算
- MME：矩阵乘法引擎
- QPU：量子处理单元
- RDU：可重构数据流单元
- TBP：总板载功率
- TDP：热设计功率或热设计参数
- TGP：总图形功率
- TPU：张量处理单元

## 最重要的一点理解

我将在本书中多次提到——仅仅购买/租赁最昂贵的加速器并不能保证高投资回报率（ROI）。

对于机器学习训练而言，两个关键的投资回报率指标是：
1. 训练完成的速度，因为如果训练比计划的时间长2-3倍，你的模型可能在发布前变得无关紧要——时间就是一切。
2. 训练模型所需的总金额，因为如果训练时间比计划长2-3倍，花费也会相应增加2-3倍。

除非所购买/租赁的其他硬件经过仔细选择以匹配所需的工作负载，否则加速器很可能会大量闲置，时间和金钱都会浪费。最关键的组件是[网络](../../network)，其次是[存储](../../storage/)，最不重要的则是[CPU](../cpu)和[CPU内存](../cpu-memory)（至少对于典型的训练工作负载而言，任何CPU限制都可以通过多个`DataLoader`工件来补偿）。

如果租用计算资源，通常没有选择的自由——硬件要么是固定的，要么某些组件可以更换但选择有限。因此，有时选择的云服务提供商可能无法提供足够匹配的硬件，在这种情况下，最好寻找其他提供商。

如果你自己购买服务器，我建议在购买之前进行深入的尽职调查。

除了硬件，当然还需要软件来高效地部署硬件。

我们将在本书的不同章节讨论硬件和软件方面的问题。你可以从[这里](../../training/performance)和[这里](../../training/model-parallelism)开始阅读。