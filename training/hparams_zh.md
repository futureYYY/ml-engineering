# hparams - 中文翻译

选择训练超参数和模型初始化

找到一组好的超参数和模型初始值的最简单方法是从一个已知成功的类似训练中借鉴。这里有一个[公开的LLM/VLM训练日志集合](../resources/README.md#publicly-available-training-llmvlm-logbooks)可供参考。另一个常见的来源是论文，如果它们披露了这些信息的话。如果没有公布，你也可以尝试联系作者询问这些细节。

## 全局批量大小逐步增加

如果你打算使用非常大的GBS（例如1024或2048样本甚至更高）进行训练，在刚开始时就给模型喂入如此大的批量大小是非常浪费的。此时数据完全是随机的，无法从中获益。因此，为了节省数据和资源，通常会随着时间逐渐增加全局批量大小。

同样重要的是，不要以过小的GBS开始，否则进度将不会高效。当数据量过小时，计算（TFLOPS）效率低下，会拖慢整个过程。特别是当使用Pipeline并行（PP）时，因为PP调整的关键在于减少GPU空闲时间，而GBS越小，这个空闲时间就越大。

例如，在BLOOM-176B的情况下，我们确实使用了PP，通过吞吐量基准测试后发现从GBS=16开始异常缓慢（8 TFLOPs），所以我们最终从GBS=192（73 TFLOPs）开始，然后逐步增加到GBS=2048（150 TFLOPs）——每9,765,625个样本增加16的GBS。

### 标准差初始化（STD Init）

这个超参数非常重要，需要进行数学计算才能正确设置。详细信息请参见[标准差初始化](instabilities#std-init)。